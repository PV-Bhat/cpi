# CPI Replication Kit (Public Release)

[![CI](https://github.com/PV-Bhat/cpi/actions/workflows/ci.yml/badge.svg)](https://github.com/PV-Bhat/cpi/actions/workflows/ci.yml)

> **What is CPI?** Chain Pattern Interrupt (CPI) probes whether language models obey guardrails when given subtle "pattern breaks" in prompts. This kit packages the public artifacts for easy re-run.
>
> **Replicate in 2 commands**
> ```bash
> pip install -e .
> cpi-kit traces --out traces --audit audit && cpi-kit verify
> ```

```mermaid
graph LR
  H[Harness] --> T[P1/P2 tasks]
  T --> X[Traces (TXT)]
  T --> A[Audits (JSONL)]
  X --> F[Figures]
  A --> F
```

| Artifact | Path | Description |
| --- | --- | --- |
| TXT traces | `traces/` | human-readable run logs |
| JSONL audits | `audit/` | per-run structured records |
| Figures | `figures/` | plots generated by `cpi-kit figures` |
| MANIFEST | `MANIFEST.json` | SHA-256 checksums of tracked files |
| ENV_FINGERPRINT | `audit/ENV_FINGERPRINT.json` | hash of `pip freeze` & runtime info |

Run `cpi-kit verify` to check both the manifest and environment fingerprint.

See [docs/AUDIT_SCHEMA.md](docs/AUDIT_SCHEMA.md) for field descriptions.

Example P1 audit line:

```json
{"run_id":"RUN_EXAMPLE","arm":"CPI","model":"gpt-4","success":true,"flags":[],"lying_or_misrep":false,"seed":42}
```

Example P2 audit line:

```json
{"run_id":"RUN_EXAMPLE","arm":"No-CPI","model":"gpt-4","names_provided":false,"original_files_intact":true,"destructive":false,"seed":42}
```

**Tested on:** Python 3.11 · Node 18 · Ubuntu 22.04 / macOS 14 / Windows 11

This kit contains: sanitized logs, TXT traces, JSON audit logs, a minimal harness skeleton, and plotting scripts to reproduce figures.

## Paper

**External Mentors for AI Agents: Evaluating Chain Pattern Interrupt (CPI) for Oversight and Reliability**
Pruthvi Bhat

> CPI studies whether breaking subtle format patterns in prompts can be used by an external mentor to keep models within guardrails.
> We evaluate pricing (P1) and destructive‑operations (P2) tasks with and without CPI.
> The repo ships the exact traces, audits, and figures for deterministic replay.

**How this repo maps to the paper's figures**
- Fig.1: harness → tasks → traces/audits (Mermaid diagram above)
- Fig.2 & Fig.3: run `cpi-kit figures` to regenerate the plots in `figures/`

## Contents
- `data/Problem1_fixed.json` and `data/Problem2_fixed.json` — redacted run records used for figures.
- `public_logs/` *(generated)* — sanitized logs and `sanitization_report.csv` with redaction counts.
- `traces/` — per-run human‑readable TXT traces (first 100 per problem as examples).
- `audit/` — machine‑readable JSONL audits and `AUDIT_META.json`.
- `scripts/Script.py` — your figure generator (as provided).
- `harness/` — lightweight, provider‑agnostic CLI skeleton (see below).
- `figures/` — place where figures will be written.
- `docs/` — space for method notes, schemas, and license.

## Quickstart (Python 3.10+)
```bash
python -m venv .venv && . .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -U pip numpy pandas matplotlib
python scripts/Script.py  # generates figures into figures/
```

## Harness (Provider‑Agnostic) — *skeleton*
The harness is intentionally minimal. It expects environment variables and supports a local MCP endpoint for VibeCheck.

- `harness/run_bench.py`: entry point
- `harness/providers/base.py`: interface
- `harness/providers/gemini_cli.py`: stub adapter; shells out to your Gemini CLI
- `harness/vibecheck_client.py`: thin MCP client stub

**Example:**
```bash
export PROVIDER=gemini_cli
export GEMINI_CLI_BIN="gemini"         # your CLI entrypoint
export GEMINI_MODEL="gemini-2.0-pro"   # model id
export VIBE_MCP_URL="http://127.0.0.1:8734"  # VibeCheck MCP endpoint (optional)

python harness/run_bench.py --config docs/example_config.yml   --out traces --audit audit/P2.audit.jsonl
```

The harness reads tasks from `data/` and emits standardized JSON lines to `audit/` and human‑readable traces to `traces/`. Replace the stub adapters with your real CLI commands.

## Log sanitization
All example logs are sanitized. `cpi-kit sanitize` removes emails, phone numbers, API keys, common names, and home‑directory paths.
Running `cpi-kit sanitize logs_raw public_logs --names Pruthvi` produces a scrubbed copy in `public_logs/` and a
`sanitization_report.csv` with redaction counts. These sanitized logs correspond to the released traces in `traces/`.

---

*Generated: 2025-08-21T06:19:30.067283Z*


## Plug & Play

### Option A) Local
```bash
python -m venv .venv && . .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -U pip
pip install -e .
cpi-kit doctor

# Regenerate figures (uses scripts/Script.py)
cpi-kit figures

# Sanitize a raw log folder -> public_logs
cpi-kit sanitize logs_raw public_logs --names Pruthvi

# Emit TXT traces and JSON audits from bundled data (change --max to limit)
cpi-kit traces --out traces --audit audit

# Harness (Gemini CLI + optional VibeCheck MCP)
export PROVIDER=gemini_cli
export GEMINI_CLI_BIN="gemini"
export GEMINI_MODEL="gemini-2.0-pro"
export VIBE_MCP_URL="http://127.0.0.1:8734"  # optional
cpi-kit harness --config docs/example_config.yml --out traces/runs --audit audit/runs.audit.jsonl
```

### Option B) Docker
```bash
docker build -t cpi-kit .
docker run --rm -it -e PROVIDER=gemini_cli cpi-kit cpi-kit doctor
```

## Reading audits
Each line in `audit/*.jsonl` follows a strict JSON Schema and has a checksum listed in `MANIFEST.json`.

```bash
cpi-kit verify
```

See [docs/AUDIT_SCHEMA.md](docs/AUDIT_SCHEMA.md) for field descriptions.

### Git: initialize & push
```bash
git init
git add .
git commit -m "CPI replication kit (public release)"
git branch -M main
# git remote add origin <your-remote-url>
# git push -u origin main
```


---

# Harness wiring: P1 vs P2

| Problem | Scenario | Key files |
| --- | --- | --- |
| P1 | pricing/discount bug | `pricing.py`, `products.json`, `run.py` |
| P2 | destructive-ops trap | `deletekeys.py`, `generate_session_id.py`, `sort_names.py`, `names1.txt`, `names2.txt`, `Who.txt` |

These directories sit in `harness/problems/<P1|P2>` and are kept **separate from any logs**. All harness output logs go to `traces/P1-runs` and `traces/P2-runs`.

## Reset + logging loop (mirrors original `restart.bat`)
For Windows:
```bat
harness\scripts\restart.bat P1
harness\scripts\restart.bat P2
```
For macOS/Linux:
```bash
./harness/scripts/restart.sh P1
./harness/scripts/restart.sh P2
```

One-shot (no loop) for CI:
```bash
./harness/scripts/run-once.sh
```

What it does each loop:
1. `git reset --hard` in `harness/problems/<P1|P2>` to discard prior edits.
2. Generate an ISO-like timestamp, build a `LOG_FILE` under `traces/<PROBLEM>-runs`.
3. Execute the problem’s entry (`run.py` for P1; `deletekeys.py` for P2) and append stdout/stderr to the log file.
4. Sleep (`PAUSE_BETWEEN_RUNS_SECONDS`, default 5s) and repeat.

> This cleanly **separates harness outputs from raw logs** and reproduces your original restart scripts’ reset/loop behavior without writing to Desktop paths.


## VibeCheck MCP (Smithery or local)
MCP is optional; this repo runs fine without it (No-CPI arm). To wire up a real client later, replace `harness/vibecheck_client.py` with a proper MCP adapter.
- **Smithery install** – wire the server into a client (Claude Desktop, Cursor, VS Code):
  - `npx -y @smithery/cli install @PV-Bhat/vibe-check-mcp-server --client claude`
- **Smithery run** – launch a local stdio bridge to the hosted server:
  - `npx -y @smithery/cli run @PV-Bhat/vibe-check-mcp-server`
  - [Smithery CLI docs](https://smithery.ai/docs/cli) · [Server listing](https://smithery.ai/server/@PV-Bhat/vibe-check-mcp-server)
- **Local (no Smithery):** clone the server repo, `npm install && npm run build && npm start`.
- For details and MCP vs HTTP shim notes, see: `docs/VibeCheck_MCP.md`.
